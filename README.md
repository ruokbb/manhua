# 应用scrapy_redis框架
- 简单使用了scrapy-redis进行分布式爬取
- 听网友说框架本身过滤器不咋地，对照着看了下源码，本质是用set集合去过滤，本来想重写过滤器，使用布隆过滤器，研究了一阵子，发现其实项目容量也不算大，也就算了
- 爬虫伪装反扒方面现在也只是用了一个随机的user_agent，IP代理池还没弄，反正现在爬的DMZJ也没什么反扒，留着当下个学期的任务量
# 对于一些异步加载，ajax，使用scrapy-splash
- 本来想直接用selenium进行一些js的编译，但是听说splash更快，一查发现还有线程的轮子，爱了。
- 当然，本质上还是解析js最稳健，好像有针对这种selenium的反扒，没见过，暂时先这样。

# 遇到的一些问题
## 1. url的全站爬取和增量爬取
- 爬取dmzj的全部漫画url，根据字母索引进行爬取，动态改变start_url，在pipline中进行监听redis列表，重置列表，但是没有考虑到scripy_redis的异步框架结构，最后只能每次只爬取一个url，极大的降低了效率
## 2. 漫画页面的爬取
- 一开始只遇到过一种页面，根据respose引用re正则进行相关信息的爬取，如作者，地区，类型等，后来看运行日志发现有报错，输入网址发现是新的网页结构，需要换一种正则。后来观察DMZJ应该是经过改版，大部分国漫和新的日本漫画都放在新的网页结构。
- 经过详细分析抓包，发现每个漫画都有一个ID，找一个API可以根据漫画ID获取json数据，基本包括漫画的全部信息，最后通过api和response的结合进行爬取。


# 最重要的ps
- 服务器的IP还在代码里，包括redis的账号密码，求放过，过段时间想个办法屏蔽掉
